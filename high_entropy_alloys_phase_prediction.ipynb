{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxL2vyRSb68A"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install ydata-profiling\n",
        "!pip -q install pandas==2.2.2 numpy==1.26.4 scikit-learn==1.5.2\n",
        "!pip -q install matplotlib==3.9.2 seaborn==0.13.2\n",
        "!pip -q install tensorflow==2.17.0\n",
        "!pip -q install imbalanced-learn==0.12.3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, warnings, numpy as np, pandas as pd\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "    from ydata_profiling import ProfileReport\n",
        "except Exception:\n",
        "    from pandas_profiling import ProfileReport  # fallback\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "tf.random.set_seed(RANDOM_STATE)\n"
      ],
      "metadata": {
        "id": "zH9e0APOiLRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "print(\"Upload your CSV nowâ€¦\")\n",
        "_ = files.upload()\n",
        "\n",
        "CSV_PATH = \"new dataset v1 - HEA Phase DataSet v1d(1).csv\"  # change if needed\n",
        "assert os.path.exists(CSV_PATH), f\"CSV not found at: {CSV_PATH}\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Shape:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "8coW9y1AiLMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"\\nMicrostructure value counts:\")\n",
        "print(df.get(\"Microstructure\", pd.Series(dtype=object)).value_counts(dropna=False).head(10))\n",
        "print(\"\\nPhases value counts:\")\n",
        "print(df[\"Phases\"].value_counts(dropna=False))\n"
      ],
      "metadata": {
        "id": "xGyls3OLiLKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_phase_to_3c(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    s = str(x).strip().upper()\n",
        "    if s == \"FCC_SS\": return \"FCC\"\n",
        "    if s == \"BCC_SS\": return \"BCC\"\n",
        "    if s in [\"FCC_PLUS_BCC\", \"IM\"]: return \"MULTI\"\n",
        "    return \"MULTI\"\n",
        "\n",
        "if \"Phases\" not in df.columns:\n",
        "    raise ValueError(\"Expected column 'Phases' not found.\")\n",
        "\n",
        "df[\"Target3\"] = df[\"Phases\"].apply(map_phase_to_3c)\n",
        "print(df[\"Target3\"].value_counts(dropna=False))\n",
        "\n",
        "df = df[~df[\"Target3\"].isna()].copy().reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "oxEScJv1iLIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_feature_like = {\n",
        "    \"Alloy ID\",\"Alloy\",\"References\",\"Reference\",\"Sythesis_Route\",\"Hot-Cold_Working\",\n",
        "    \"IM_Structure\",\"Microstructure_\",\"Microstructure\",\"Phases\",\"Target3\",\n",
        "    \"Multiphase\",\"HPR\",\"Quenching\"\n",
        "}\n",
        "more_non_feats = {\"Homogenization_Temp\",\"Homogenization_Time\",\"Annealing_Temp\",\"Annealing_Time_(min)\"}\n",
        "drop_cols = non_feature_like.union(more_non_feats)\n",
        "\n",
        "num_df = df.select_dtypes(include=[np.number]).copy()\n",
        "feature_cols = [c for c in num_df.columns if c not in drop_cols]\n",
        "\n",
        "ELEMENTS = [\"Al\",\"Co\",\"Cr\",\"Fe\",\"Ni\",\"Cu\",\"Mn\",\"Ti\",\"Zr\",\"Nb\",\"Mo\",\"Ta\",\"V\",\"Hf\",\"W\",\"Si\",\"C\",\"N\",\"B\"]\n",
        "composition_cols = [c for c in feature_cols if c in ELEMENTS]\n",
        "\n",
        "extra_feats = [c for c in feature_cols if c in\n",
        "               [\"Hmix\",\"Sconf\",\"Omega\",\"Delta\",\"VEC\",\"Atom.Size.Diff\",\"Elect.Diff\",\"rA/rX\"]]\n",
        "\n",
        "chosen_features = sorted(list(set(composition_cols + extra_feats))) or feature_cols\n",
        "print(f\"{len(chosen_features)} features selected:\\n\", chosen_features)\n",
        "\n",
        "X_all = df[chosen_features].copy()\n",
        "y_all = df[\"Target3\"].copy()\n"
      ],
      "metadata": {
        "id": "6MwFZnpeiLGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing values (top 20):\")\n",
        "print(X_all.isna().sum().sort_values(ascending=False).head(20))\n",
        "\n",
        "display(X_all.describe(percentiles=[0.01,0.05,0.25,0.5,0.75,0.95,0.99]).T)\n"
      ],
      "metadata": {
        "id": "2-j7DTgTiLDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(min(len(chosen_features)+4, 18), min(len(chosen_features)+4, 18)))\n",
        "corr = X_all.corr(numeric_only=True)\n",
        "sns.heatmap(corr, annot=False, cmap=\"coolwarm\")\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.tight_layout(); plt.show()\n",
        "profile = ProfileReport(df[chosen_features + [\"Target3\"]],\n",
        "                        title=\"HEA Profiling (selected features)\", explorative=True, minimal=True)\n",
        "profile.to_file(\"hea_profile_report.html\")\n",
        "print(\"Saved: hea_profile_report.html\")\n"
      ],
      "metadata": {
        "id": "0pCFVhRViLBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y_all)\n",
        "print(\"Classes:\", list(le.classes_))  # expected ['BCC','FCC','MULTI']\n",
        "\n",
        "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
        "    X_all, y_encoded, test_size=0.30, random_state=RANDOM_STATE, stratify=y_encoded\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_tmp, y_tmp, test_size=0.50, random_state=RANDOM_STATE, stratify=y_tmp\n",
        ")\n",
        "print(\"Shapes ->\", X_train.shape, X_val.shape, X_test.shape)\n"
      ],
      "metadata": {
        "id": "D1Sg8OSeiK_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "scaler  = StandardScaler()\n",
        "\n",
        "X_train_imp = imputer.fit_transform(X_train)\n",
        "X_val_imp   = imputer.transform(X_val)\n",
        "X_test_imp  = imputer.transform(X_test)\n",
        "\n",
        "X_train_s = scaler.fit_transform(X_train_imp)\n",
        "X_val_s   = scaler.transform(X_val_imp)\n",
        "X_test_s  = scaler.transform(X_test_imp)\n",
        "\n",
        "input_dim   = X_train_s.shape[1]\n",
        "num_classes = len(np.unique(y_encoded))\n",
        "\n",
        "dump(imputer, \"imputer.joblib\"); dump(scaler, \"scaler.joblib\"); dump(le, \"label_encoder.joblib\")\n",
        "print(\"Preprocessors saved.\")\n"
      ],
      "metadata": {
        "id": "y5VLHGy7iK8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = np.unique(y_train)\n",
        "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
        "class_weights = {int(c): float(w) for c, w in zip(classes, cw)}\n",
        "class_weights\n"
      ],
      "metadata": {
        "id": "KHJ_2KZjiK6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_dim, num_classes):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.30),\n",
        "\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.20),\n",
        "\n",
        "        layers.Dense(num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "model = build_model(input_dim, num_classes)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "C33LQ4RaiK4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=25,\n",
        "                                     restore_best_weights=True, mode=\"max\")\n",
        "reduce_lr  = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5,\n",
        "                                         patience=10, verbose=1, min_lr=1e-6)\n",
        "checkpoint = callbacks.ModelCheckpoint(\"best_model.keras\", monitor=\"val_accuracy\",\n",
        "                                       save_best_only=True, mode=\"max\")\n",
        "\n",
        "BATCH, EPOCHS = 64, 300\n",
        "history = model.fit(\n",
        "    X_train_s, y_train,\n",
        "    validation_data=(X_val_s, y_val),\n",
        "    epochs=EPOCHS, batch_size=BATCH,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "JmhIoYqOiK18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def plot_cm(y_true, y_pred, labels, title):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=range(len(labels)))\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(title)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "print(\"Validation:\")\n",
        "val_preds = np.argmax(model.predict(X_val_s), axis=1)\n",
        "print(\"Val Acc:\", round(accuracy_score(y_val, val_preds)*100, 2), \"%\")\n",
        "print(classification_report(y_val, val_preds, target_names=le.classes_))\n",
        "plot_cm(y_val, val_preds, le.classes_, \"Validation CM\")\n",
        "\n",
        "print(\"Test:\")\n",
        "test_preds = np.argmax(model.predict(X_test_s), axis=1)\n",
        "print(\"Test Acc:\", round(accuracy_score(y_test, test_preds)*100, 2), \"%\")\n",
        "print(classification_report(y_test, test_preds, target_names=le.classes_))\n",
        "plot_cm(y_test, test_preds, le.classes_, \"Test CM\")\n"
      ],
      "metadata": {
        "id": "rSsNmFM-iKzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"final_model.keras\")\n",
        "print(\"Saved final_model.keras\")\n",
        "\n",
        "inference_py = r'''\n",
        "import joblib, numpy as np, pandas as pd, tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "imputer = joblib.load(\"imputer.joblib\")\n",
        "scaler  = joblib.load(\"scaler.joblib\")\n",
        "le      = joblib.load(\"label_encoder.joblib\")\n",
        "model   = keras.models.load_model(\"final_model.keras\")\n",
        "\n",
        "EXPECTED = {cols}\n",
        "\n",
        "def predict_phase(df_features: pd.DataFrame):\n",
        "    missing = set(EXPECTED) - set(df_features.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing features: {missing}\")\n",
        "    X = df_features[list(EXPECTED)].copy()\n",
        "    X_imp = imputer.transform(X)\n",
        "    X_scl = scaler.transform(X_imp)\n",
        "    probs = model.predict(X_scl)\n",
        "    preds = probs.argmax(axis=1)\n",
        "    labels = le.inverse_transform(preds)\n",
        "    return labels, probs\n",
        "'''.format(cols=json.dumps(chosen_features))\n",
        "\n",
        "with open(\"predict_helper.py\",\"w\") as f:\n",
        "    f.write(inference_py)\n",
        "print(\"Wrote predict_helper.py\")\n"
      ],
      "metadata": {
        "id": "xusOnNeqiKsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex = pd.DataFrame([X_train.median().to_dict()])  # fake row\n",
        "from joblib import load\n",
        "imputer = load(\"imputer.joblib\"); scaler = load(\"scaler.joblib\")\n",
        "ex_pred = np.argmax(model.predict(scaler.transform(imputer.transform(ex))), axis=1)\n",
        "print(\"Example prediction:\", [LabelEncoder().fit(le.classes_).inverse_transform(ex_pred)[0]])\n"
      ],
      "metadata": {
        "id": "jkR7fqWrip-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uahw_l_Zip71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sWVJnAbfip5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vihUm7cZip3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9YczZunniKei"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}